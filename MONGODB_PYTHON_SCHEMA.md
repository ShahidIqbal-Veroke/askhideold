# üêç MongoDB Schema Design - Python Backend

## Architecture NoSQL Risk-Centric

Ce design MongoDB est optimis√© pour votre architecture **risk-centric** avec Python backend. MongoDB excelle dans la gestion des corr√©lations complexes, documents JSON riches, et requ√™tes de scoring en temps r√©el.

---

## üèóÔ∏è Collections MongoDB

### **Collection Principale : `risks`** (Hub Central)

```python
# models/risk.py
from pydantic import BaseModel, Field
from typing import Dict, List, Optional, Any
from datetime import datetime
from enum import Enum
from bson import ObjectId

class RiskLevel(str, Enum):
    VERY_LOW = "very_low"
    LOW = "low" 
    MEDIUM = "medium"
    HIGH = "high"
    VERY_HIGH = "very_high"
    CRITICAL = "critical"

class RiskType(str, Enum):
    FRAUDE_DOCUMENTAIRE = "fraude_documentaire"
    FRAUDE_IDENTITE = "fraude_identite"
    FRAUDE_SINISTRE = "fraude_sinistre"
    FRAUDE_SOUSCRIPTION = "fraude_souscription"
    BLANCHIMENT = "blanchiment"
    CYBER_FRAUDE = "cyber_fraude"

class RiskStatus(str, Enum):
    DETECTED = "detected"
    INVESTIGATING = "investigating"
    MITIGATED = "mitigated"
    ACCEPTED = "accepted"
    TRANSFERRED = "transferred"
    CLOSED = "closed"

class RiskCorrelation(BaseModel):
    risk_id: str = Field(..., description="ObjectId du risque corr√©l√©")
    correlation_type: str = Field(..., description="causal, temporal, behavioral, contextual")
    strength: float = Field(..., ge=0, le=1, description="Force de la corr√©lation 0-1")
    confidence: float = Field(..., ge=0, le=1, description="Confiance dans la corr√©lation")
    detected_at: datetime = Field(default_factory=datetime.now)
    analysis: Dict[str, Any] = Field(default_factory=dict)
    impact: Dict[str, Any] = Field(default_factory=dict)

class RiskScoring(BaseModel):
    base_score: int = Field(..., ge=0, le=100)
    adjusted_score: int = Field(..., ge=0, le=100)
    final_score: int = Field(..., ge=0, le=100)
    confidence: float = Field(..., ge=0, le=1)
    component_scores: Dict[str, float] = Field(default_factory=dict)
    adjustment_factors: Dict[str, float] = Field(default_factory=dict)
    quality_metrics: Dict[str, Any] = Field(default_factory=dict)

class Risk(BaseModel):
    id: Optional[str] = Field(default=None, alias="_id")
    
    # Relations (ObjectId r√©f√©rences)
    assure_id: str = Field(..., description="R√©f√©rence vers assures collection")
    cycle_vie_id: Optional[str] = Field(None)
    historique_id: Optional[str] = Field(None)
    evenement_id: Optional[str] = Field(None)
    
    # Classification
    type: RiskType
    category: str = Field(..., description="fraud, aml, kyc, operational, financial")
    level: RiskLevel
    status: RiskStatus = RiskStatus.DETECTED
    source: str = Field(..., description="document_analysis, behavioral_analysis, etc.")
    
    # Description
    title: str = Field(..., max_length=500)
    description: str
    short_description: str = Field(..., max_length=255)
    
    # Scoring Central (Hub de calcul)
    scoring: RiskScoring
    
    # Facteurs de risque sp√©cifiques au type
    risk_factors: Dict[str, Any] = Field(default_factory=dict)
    
    # √âvaluation d'impact
    potential_impact: Dict[str, Any] = Field(default_factory=dict)
    
    # Contexte m√©tier
    business_context: Dict[str, Any] = Field(default_factory=dict)
    
    # Entit√©s li√©es (r√©f√©rences flexibles)
    related_entities: Dict[str, List[str]] = Field(default_factory=dict)
    
    # Preuves et √©vidence
    evidence: Dict[str, Any] = Field(default_factory=dict)
    
    # Actions de mitigation
    mitigation: Dict[str, Any] = Field(default_factory=dict)
    
    # Monitoring et seuils
    monitoring: Dict[str, Any] = Field(default_factory=dict)
    
    # Corr√©lations avec autres risques (Hub de corr√©lation)
    correlations: List[RiskCorrelation] = Field(default_factory=list)
    
    # Pr√©dictions ML
    predictions: Dict[str, Any] = Field(default_factory=dict)
    
    # Workflow
    requires_approval: bool = False
    approved_by: Optional[str] = None
    approved_at: Optional[datetime] = None
    escalated_to: Optional[str] = None
    escalated_at: Optional[datetime] = None
    
    # Audit trail
    created_at: datetime = Field(default_factory=datetime.now)
    created_by: str
    updated_at: datetime = Field(default_factory=datetime.now)
    last_review_at: Optional[datetime] = None
    last_review_by: Optional[str] = None
    
    # Versioning
    version: int = 1
    score_history: List[Dict[str, Any]] = Field(default_factory=list)
    
    # Tags et m√©tadonn√©es
    tags: List[str] = Field(default_factory=list)
    custom_fields: Dict[str, Any] = Field(default_factory=dict)
    
    class Config:
        allow_population_by_field_name = True
        arbitrary_types_allowed = True
```

### **Collection : `assures`**

```python
# models/assure.py
from pydantic import BaseModel, Field
from typing import Dict, List, Optional, Any
from datetime import datetime
from enum import Enum

class AssureType(str, Enum):
    PARTICULIER = "particulier"
    PROFESSIONNEL = "professionnel" 
    ENTREPRISE = "entreprise"

class AssureStatus(str, Enum):
    PROSPECT = "prospect"
    ACTIVE = "active"
    SUSPENDED = "suspended"
    TERMINATED = "terminated"

class RiskProfile(BaseModel):
    risk_score: int = Field(default=50, ge=0, le=100)
    risk_level: str = Field(default="medium")
    risk_confidence: float = Field(default=0.5, ge=0, le=1)
    
    # M√©triques de risque calcul√©es
    nombre_sinistres: int = 0
    montant_total_sinistres: float = 0
    nombre_alertes: int = 0
    nombre_dossiers_fraude: int = 0
    frequence_modifications: int = 0
    delai_declaration_moyen: int = 0  # jours
    coherence_documents: int = Field(default=100, ge=0, le=100)
    
    # Facteurs de risque
    facteurs_durcissement: List[str] = Field(default_factory=list)
    facteurs_mitigation: List[str] = Field(default_factory=list)
    
    # Derni√®res mises √† jour
    derniere_mise_a_jour_risque: datetime = Field(default_factory=datetime.now)
    prochaine_review: Optional[datetime] = None

class Assure(BaseModel):
    id: Optional[str] = Field(default=None, alias="_id")
    
    # Identifiants m√©tier
    numero_client: str = Field(..., description="Identifiant unique m√©tier")
    type: AssureType
    status: AssureStatus = AssureStatus.PROSPECT
    
    # Informations identit√© (JSON flexible)
    identity: Dict[str, Any] = Field(..., description="nom, prenom, raison_sociale, etc.")
    
    # Profil de risque (calcul√© depuis les risques)
    risk_profile: RiskProfile = Field(default_factory=RiskProfile)
    
    # Portfolio et m√©triques business
    total_premiums: float = 0
    customer_lifetime_value: float = 0
    lines_of_business: List[str] = Field(default_factory=list)
    anciennete_client: int = 0  # mois
    
    # Assignation
    gestionnaire: Optional[str] = None
    agence: Optional[str] = None
    segment: Optional[str] = None
    
    # Relations vers autres collections
    contract_ids: List[str] = Field(default_factory=list)
    cycle_vie_ids: List[str] = Field(default_factory=list)
    evenement_ids: List[str] = Field(default_factory=list)
    risk_ids: List[str] = Field(default_factory=list)  # Principaux risques
    alert_ids: List[str] = Field(default_factory=list)
    case_ids: List[str] = Field(default_factory=list)
    
    # Audit trail
    created_at: datetime = Field(default_factory=datetime.now)
    created_by: str
    updated_at: datetime = Field(default_factory=datetime.now)
    last_login_at: Optional[datetime] = None
    data_validated_at: Optional[datetime] = None
    
    class Config:
        allow_population_by_field_name = True
```

### **Collection : `evenements`** (Ex-Demandes)

```python
# models/evenement.py
from pydantic import BaseModel, Field
from typing import Dict, List, Optional, Any
from datetime import datetime
from enum import Enum

class EvenementType(str, Enum):
    # Contractuels
    SOUSCRIPTION_CONTRAT = "souscription_contrat"
    MODIFICATION_CONTRAT = "modification_contrat" 
    RENOUVELLEMENT = "renouvellement"
    RESILIATION = "resiliation"
    
    # Sinistres
    DECLARATION_SINISTRE = "declaration_sinistre"
    COMPLEMENT_SINISTRE = "complement_sinistre"
    CONTESTATION_DECISION = "contestation_decision"
    
    # Service Client
    DEMANDE_INFO = "demande_info"
    RECLAMATION = "reclamation"
    ATTESTATION = "attestation"
    
    # D√©tection Fraude
    PATTERN_DETECTION = "pattern_detection"
    BEHAVIOR_ANOMALY = "behavior_anomaly"
    DOCUMENT_INCONSISTENCY = "document_inconsistency"

class EvenementStatus(str, Enum):
    RECEIVED = "received"
    IN_PROGRESS = "in_progress" 
    COMPLETED = "completed"
    REJECTED = "rejected"
    ESCALATED = "escalated"

class Evenement(BaseModel):
    id: Optional[str] = Field(default=None, alias="_id")
    
    # Identifiants
    reference_externe: Optional[str] = None
    numero_suivi: str = Field(..., description="Num√©ro de suivi unique")
    
    # Classification
    type: EvenementType
    category: str
    status: EvenementStatus = EvenementStatus.RECEIVED
    priority: str = Field(default="medium")
    
    # Origine
    origin: str
    channel: str
    source_info: Dict[str, Any] = Field(default_factory=dict)
    
    # Demandeur
    demandeur_info: Dict[str, Any]
    assure_id: Optional[str] = None  # Si li√© √† un assur√© existant
    
    # Contenu
    objet: str = Field(..., max_length=500)
    description: str
    motivation: Optional[str] = None
    
    # Donn√©es sp√©cifiques au type (JSON flexible)
    donnees_specifiques: Dict[str, Any] = Field(default_factory=dict)
    
    # Contexte m√©tier
    contexte_business: Dict[str, Any] = Field(default_factory=dict)
    
    # Documents attach√©s
    documents: List[Dict[str, Any]] = Field(default_factory=list)
    
    # Workflow
    workflow_state: Dict[str, Any] = Field(default_factory=dict)
    
    # SLA et d√©lais
    date_reception: datetime = Field(default_factory=datetime.now)
    delai_commercial: int  # jours
    date_echeance: datetime
    date_traitement: Optional[datetime] = None
    respect_sla: bool = True
    
    # Assignation et traitement
    assigne_a: Optional[str] = None
    equipe_traitante: Optional[str] = None
    date_assignation: Optional[datetime] = None
    historique_traitement: List[Dict[str, Any]] = Field(default_factory=list)
    
    # D√©cision
    decision_type: Optional[str] = None
    decision_motif: Optional[str] = None
    montant_accorde: Optional[float] = None
    date_decision: Optional[datetime] = None
    decideur: Optional[str] = None
    
    # M√©triques qualit√©
    note_complexite: Optional[int] = Field(None, ge=1, le=5)
    temps_traitement: Optional[int] = None  # minutes
    nombre_aller_retours: int = 0
    
    # Analytics
    score_urgence: int = Field(default=50, ge=0, le=100)
    score_complexite: int = Field(default=50, ge=0, le=100)
    impact_business: int = Field(default=50, ge=0, le=100)
    cout_traitement: float = 0
    
    # Relations g√©n√©r√©es
    historique_id: Optional[str] = None  # √âv√©nement historique cr√©√©
    risk_ids: List[str] = Field(default_factory=list)  # Risques d√©tect√©s
    
    # Audit trail
    created_at: datetime = Field(default_factory=datetime.now)
    created_by: str
    updated_at: datetime = Field(default_factory=datetime.now)
    
    class Config:
        allow_population_by_field_name = True
```

### **Collection : `alerts`** (G√©n√©r√©es par les Risques)

```python
# models/alert.py
from pydantic import BaseModel, Field
from typing import Dict, List, Optional, Any
from datetime import datetime
from enum import Enum

class AlertSeverity(str, Enum):
    CRITICAL = "critical"
    HIGH = "high" 
    MEDIUM = "medium"
    LOW = "low"

class AlertStatus(str, Enum):
    PENDING = "pending"
    ASSIGNED = "assigned"
    IN_REVIEW = "in_review"
    QUALIFIED = "qualified" 
    REJECTED = "rejected"

class AlertQualification(str, Enum):
    FRAUD_CONFIRMED = "fraud_confirmed"
    FALSE_POSITIVE = "false_positive"
    REQUIRES_INVESTIGATION = "requires_investigation"

class Alert(BaseModel):
    id: Optional[str] = Field(default=None, alias="_id")
    
    # Source
    external_id: Optional[str] = None
    source: str
    source_module: str
    
    # Classification (pilot√©e par le risque)
    type: str
    severity: AlertSeverity
    score: int = Field(..., ge=0, le=100)
    confidence: float = Field(..., ge=0, le=1)
    
    # Relation avec le risque (Hub central)
    risk_id: str = Field(..., description="Risque qui a g√©n√©r√© cette alerte")
    assure_id: str
    
    # M√©tadonn√©es enrichies
    metadata: Dict[str, Any] = Field(..., description="Contexte m√©tier riche")
    
    # Workflow
    status: AlertStatus = AlertStatus.PENDING
    qualification: Optional[AlertQualification] = None
    qualification_notes: Optional[str] = None
    
    # Assignation
    assigned_to: Optional[str] = None
    assigned_by: Optional[str] = None
    assigned_at: Optional[datetime] = None
    team: Optional[str] = None
    
    # Donn√©es techniques
    raw_data: Dict[str, Any] = Field(default_factory=dict)
    enriched_data: Dict[str, Any] = Field(default_factory=dict)
    
    # Audit trail
    created_at: datetime = Field(default_factory=datetime.now)
    created_by: Optional[str] = None
    updated_at: datetime = Field(default_factory=datetime.now)
    qualified_at: Optional[datetime] = None
    qualified_by: Optional[str] = None
    
    class Config:
        allow_population_by_field_name = True
```

---

## üîç Index MongoDB Optimis√©s

### **Index sur Collection `risks` (Hub Central)**

```python
# database/indexes.py
from pymongo import MongoClient, ASCENDING, DESCENDING, TEXT

def create_risk_indexes(db):
    risks = db.risks
    
    # Index primaires pour le hub central
    risks.create_index([("final_score", DESCENDING)], name="risk_score_desc")
    risks.create_index([("level", ASCENDING)], name="risk_level")
    risks.create_index([("status", ASCENDING)], name="risk_status")
    risks.create_index([("assure_id", ASCENDING)], name="risk_assure")
    
    # Index pour corr√©lations (critique pour performance)
    risks.create_index([
        ("correlations.risk_id", ASCENDING),
        ("correlations.strength", DESCENDING)
    ], name="risk_correlations")
    
    # Index compos√© pour recherche complexe
    risks.create_index([
        ("level", ASCENDING),
        ("status", ASCENDING),
        ("scoring.final_score", DESCENDING)
    ], name="risk_search_composite")
    
    # Index texte pour recherche full-text
    risks.create_index([
        ("title", TEXT),
        ("description", TEXT),
        ("tags", TEXT)
    ], name="risk_text_search")
    
    # Index temporel pour trending
    risks.create_index([
        ("created_at", DESCENDING),
        ("level", ASCENDING)
    ], name="risk_temporal")
    
    # Index g√©o-spatial si coordonn√©es dans business_context
    risks.create_index([("business_context.location", "2dsphere")], 
                      name="risk_geospatial", sparse=True)

def create_assure_indexes(db):
    assures = db.assures
    
    # Index m√©tier
    assures.create_index([("numero_client", ASCENDING)], 
                        name="assure_numero_client", unique=True)
    assures.create_index([("risk_profile.risk_score", DESCENDING)], 
                        name="assure_risk_score")
    assures.create_index([("gestionnaire", ASCENDING)], 
                        name="assure_gestionnaire")
    
    # Index pour recherche dans identity (JSON)
    assures.create_index([("identity.nom", ASCENDING)], 
                        name="assure_nom", sparse=True)
    assures.create_index([("identity.email", ASCENDING)], 
                        name="assure_email", sparse=True)

def create_evenement_indexes(db):
    evenements = db.evenements
    
    # Index workflow
    evenements.create_index([("status", ASCENDING)], name="evenement_status")
    evenements.create_index([("priority", ASCENDING)], name="evenement_priority") 
    evenements.create_index([("assigne_a", ASCENDING)], name="evenement_assigned")
    
    # Index SLA critique
    evenements.create_index([
        ("date_echeance", ASCENDING),
        ("status", ASCENDING)
    ], name="evenement_sla")
    
    # Index pour analytics
    evenements.create_index([
        ("score_urgence", DESCENDING),
        ("score_complexite", DESCENDING)
    ], name="evenement_scores")

def create_alert_indexes(db):
    alerts = db.alerts
    
    # Index par risque (relation centrale)
    alerts.create_index([("risk_id", ASCENDING)], name="alert_risk")
    alerts.create_index([("severity", DESCENDING)], name="alert_severity")
    alerts.create_index([("status", ASCENDING)], name="alert_status")
    alerts.create_index([("assigned_to", ASCENDING)], name="alert_assigned")
    
    # Index temporel pour SLA
    alerts.create_index([("created_at", DESCENDING)], name="alert_created")
```

---

## üêç Services Python

### **Service Principal : RiskService**

```python
# services/risk_service.py
from typing import List, Optional, Dict, Any
from models.risk import Risk, RiskCorrelation, RiskLevel
from models.alert import Alert
from database.mongodb import get_database
from bson import ObjectId
import numpy as np
from datetime import datetime, timedelta

class RiskService:
    def __init__(self):
        self.db = get_database()
        self.risks = self.db.risks
        self.assures = self.db.assures
        self.alerts = self.db.alerts
        
    async def calculate_risk_score(self, risk_data: Dict[str, Any]) -> Dict[str, int]:
        """Moteur de calcul de risque centralis√©"""
        
        # Score de base selon le type
        base_scores = {
            "fraude_documentaire": 70,
            "fraude_identite": 85,
            "fraude_sinistre": 60,
            "blanchiment": 90
        }
        
        base_score = base_scores.get(risk_data.get("type"), 50)
        
        # Ajustements temporels
        time_factor = self._calculate_temporal_weight(risk_data)
        
        # Ajustements corr√©lation
        correlation_boost = await self._calculate_correlation_boost(
            risk_data.get("assure_id")
        )
        
        # Ajustements historiques
        historical_factor = await self._get_historical_factor(
            risk_data.get("assure_id")
        )
        
        # Calcul final
        adjusted_score = base_score + time_factor + historical_factor
        final_score = min(100, max(0, adjusted_score + correlation_boost))
        
        return {
            "base_score": base_score,
            "adjusted_score": adjusted_score,
            "final_score": final_score
        }
    
    async def detect_correlations(self, risk_id: str) -> List[RiskCorrelation]:
        """D√©tection de corr√©lations avec autres risques"""
        
        risk = await self.get_risk_by_id(risk_id)
        if not risk:
            return []
            
        # Recherche de risques similaires
        similar_risks = await self.risks.find({
            "_id": {"$ne": ObjectId(risk_id)},
            "assure_id": risk.assure_id,
            "level": {"$in": ["high", "very_high", "critical"]},
            "created_at": {
                "$gte": datetime.now() - timedelta(days=90)
            }
        }).to_list(length=50)
        
        correlations = []
        
        for similar_risk in similar_risks:
            # Calcul de corr√©lation comportementale
            behavioral_strength = self._calculate_behavioral_correlation(
                risk, similar_risk
            )
            
            # Calcul de corr√©lation temporelle  
            temporal_strength = self._calculate_temporal_correlation(
                risk, similar_risk
            )
            
            # Force globale
            overall_strength = (behavioral_strength * 0.6 + 
                              temporal_strength * 0.4)
            
            if overall_strength > 0.3:  # Seuil de corr√©lation significative
                correlation = RiskCorrelation(
                    risk_id=str(similar_risk["_id"]),
                    correlation_type="behavioral",
                    strength=overall_strength,
                    confidence=min(behavioral_strength, temporal_strength),
                    analysis={
                        "behavioral_score": behavioral_strength,
                        "temporal_score": temporal_strength,
                        "pattern_match": True
                    }
                )
                correlations.append(correlation)
        
        return correlations
    
    async def generate_alerts_from_risk(self, risk: Risk) -> List[str]:
        """G√©n√®re des alertes bas√©es sur les seuils de risque"""
        
        alert_ids = []
        
        # Seuils pour g√©n√©ration d'alertes
        if risk.scoring.final_score >= 80:
            severity = "critical"
        elif risk.scoring.final_score >= 60:
            severity = "high"
        elif risk.scoring.final_score >= 40:
            severity = "medium"
        else:
            return []  # Pas d'alerte pour risque faible
            
        # Cr√©ation de l'alerte
        alert = Alert(
            source="risk_engine",
            source_module="risk_service",
            type=f"risk_{risk.type}",
            severity=severity,
            score=risk.scoring.final_score,
            confidence=risk.scoring.confidence,
            risk_id=risk.id,
            assure_id=risk.assure_id,
            metadata={
                "risk_level": risk.level,
                "correlation_count": len(risk.correlations),
                "business_context": risk.business_context,
                "evidence_summary": risk.evidence
            },
            created_by="system"
        )
        
        # Sauvegarde en base
        result = await self.alerts.insert_one(alert.dict(by_alias=True))
        alert_ids.append(str(result.inserted_id))
        
        return alert_ids
    
    async def update_assure_risk_profile(self, assure_id: str):
        """Met √† jour le profil de risque agr√©g√© de l'assur√©"""
        
        # R√©cup√©ration de tous les risques de l'assur√©
        risks = await self.risks.find({
            "assure_id": assure_id,
            "status": {"$nin": ["closed", "transferred"]}
        }).to_list(length=None)
        
        if not risks:
            return
            
        # Calcul du score agr√©g√©
        risk_scores = [r["scoring"]["final_score"] for r in risks]
        avg_score = int(np.mean(risk_scores))
        max_score = max(risk_scores)
        
        # Niveau de risque bas√© sur le score max
        if max_score >= 80:
            risk_level = "critical"
        elif max_score >= 60:
            risk_level = "high"  
        elif max_score >= 40:
            risk_level = "medium"
        else:
            risk_level = "low"
            
        # Mise √† jour du profil
        await self.assures.update_one(
            {"_id": ObjectId(assure_id)},
            {
                "$set": {
                    "risk_profile.risk_score": avg_score,
                    "risk_profile.risk_level": risk_level,
                    "risk_profile.nombre_alertes": len([r for r in risks if r.get("alert_ids")]),
                    "risk_profile.derniere_mise_a_jour_risque": datetime.now()
                }
            }
        )
    
    def _calculate_temporal_weight(self, risk_data: Dict) -> float:
        """Calcul du poids temporel"""
        # Plus r√©cent = plus de poids
        created_at = risk_data.get("created_at", datetime.now())
        hours_ago = (datetime.now() - created_at).total_seconds() / 3600
        
        if hours_ago <= 24:
            return 10  # Tr√®s r√©cent
        elif hours_ago <= 168:  # 1 semaine
            return 5
        else:
            return 0
    
    async def _calculate_correlation_boost(self, assure_id: str) -> float:
        """Boost bas√© sur corr√©lations existantes"""
        
        correlations = await self.risks.find({
            "assure_id": assure_id,
            "correlations": {"$exists": True, "$not": {"$size": 0}}
        }).to_list(length=10)
        
        if not correlations:
            return 0
            
        # Plus de corr√©lations = boost plus √©lev√©
        total_correlations = sum(len(r.get("correlations", [])) for r in correlations)
        return min(20, total_correlations * 2)  # Max 20 points de boost
```

---

## üöÄ API FastAPI

### **Endpoints Risk-Centric**

```python
# api/risk_routes.py
from fastapi import APIRouter, HTTPException, Depends
from typing import List, Optional
from models.risk import Risk, RiskLevel
from services.risk_service import RiskService

router = APIRouter(prefix="/api/risks", tags=["risks"])

@router.get("/correlations/{risk_id}")
async def get_risk_correlations(
    risk_id: str,
    risk_service: RiskService = Depends()
):
    """R√©cup√®re les corr√©lations d'un risque"""
    correlations = await risk_service.detect_correlations(risk_id)
    return {"correlations": correlations}

@router.get("/assure/{assure_id}/risks")
async def get_assure_risks(
    assure_id: str,
    level: Optional[RiskLevel] = None,
    limit: int = 50,
    risk_service: RiskService = Depends()
):
    """R√©cup√®re tous les risques d'un assur√©"""
    
    query = {"assure_id": assure_id}
    if level:
        query["level"] = level
        
    risks = await risk_service.risks.find(query).limit(limit).to_list(length=None)
    return {"risks": risks}

@router.post("/calculate-score")
async def calculate_risk_score(
    risk_data: dict,
    risk_service: RiskService = Depends()
):
    """Calcule le score de risque pour des donn√©es"""
    scoring = await risk_service.calculate_risk_score(risk_data)
    return {"scoring": scoring}

@router.get("/high-priority")
async def get_high_priority_risks(
    limit: int = 20,
    risk_service: RiskService = Depends()
):
    """Risques haute priorit√© n√©cessitant action imm√©diate"""
    
    risks = await risk_service.risks.find({
        "level": {"$in": ["high", "very_high", "critical"]},
        "status": {"$in": ["detected", "investigating"]},
        "requires_approval": True
    }).sort("scoring.final_score", -1).limit(limit).to_list(length=None)
    
    return {"high_priority_risks": risks}
```

---

## üîÑ Aggregation Pipelines

### **Pipeline de Corr√©lation Complexe**

```python
# aggregations/risk_correlations.py
def build_correlation_pipeline(assure_id: str):
    """Pipeline pour d√©tecter corr√©lations complexes"""
    
    return [
        # √âtape 1: Filtrer les risques de l'assur√©
        {
            "$match": {
                "assure_id": assure_id,
                "status": {"$nin": ["closed", "transferred"]}
            }
        },
        
        # √âtape 2: Lookup pour enrichir avec donn√©es assur√©
        {
            "$lookup": {
                "from": "assures",
                "localField": "assure_id", 
                "foreignField": "_id",
                "as": "assure_data"
            }
        },
        
        # √âtape 3: Groupement par patterns
        {
            "$group": {
                "_id": {
                    "type": "$type",
                    "level": "$level",
                    "source": "$source"
                },
                "risks": {"$push": "$$ROOT"},
                "count": {"$sum": 1},
                "avg_score": {"$avg": "$scoring.final_score"},
                "max_score": {"$max": "$scoring.final_score"},
                "total_confidence": {"$avg": "$scoring.confidence"}
            }
        },
        
        # √âtape 4: Filtrer patterns significatifs
        {
            "$match": {
                "count": {"$gte": 2},  # Au moins 2 risques similaires
                "avg_score": {"$gte": 40}  # Score moyen significatif
            }
        },
        
        # √âtape 5: Calcul corr√©lation strength
        {
            "$addFields": {
                "correlation_strength": {
                    "$multiply": [
                        {"$divide": ["$avg_score", 100]},
                        {"$divide": ["$count", 10]},  # Normalise par nombre
                        "$total_confidence"
                    ]
                }
            }
        },
        
        # √âtape 6: Tri par force de corr√©lation
        {
            "$sort": {"correlation_strength": -1}
        }
    ]

def build_risk_dashboard_pipeline(user_role: str):
    """Pipeline pour dashboard utilisateur selon r√¥le"""
    
    base_pipeline = [
        # Groupement par niveau de risque
        {
            "$group": {
                "_id": "$level",
                "count": {"$sum": 1},
                "avg_score": {"$avg": "$scoring.final_score"},
                "recent_count": {
                    "$sum": {
                        "$cond": [
                            {"$gte": ["$created_at", datetime.now() - timedelta(days=7)]},
                            1, 0
                        ]
                    }
                }
            }
        }
    ]
    
    # Filtres selon le r√¥le
    if user_role == "gestionnaire":
        # Filtrer sur les risques assign√©s
        match_stage = {
            "$match": {
                "$or": [
                    {"assigned_to": user_role},
                    {"status": "detected"}  # Nouveaux risques
                ]
            }
        }
        base_pipeline.insert(0, match_stage)
        
    elif user_role == "direction":
        # Vue globale avec seuils √©lev√©s
        match_stage = {
            "$match": {
                "level": {"$in": ["high", "very_high", "critical"]}
            }
        }
        base_pipeline.insert(0, match_stage)
    
    return base_pipeline
```

---

## üíæ Comparaison : PostgreSQL vs MongoDB

| Aspect | PostgreSQL (Relationnel) | MongoDB (NoSQL) ‚úÖ |
|--------|--------------------------|-------------------|
| **Corr√©lations Complexes** | JOINs multiples lents | Embedded documents rapides |
| **Scoring Temps R√©el** | Calculs lourds | Aggregation pipelines optimis√©s |
| **Schema Flexibility** | Rigide | Adaptatif aux nouveaux patterns |
| **JSON Metadata** | Support limit√© | Natif et performant |
| **Scaling Horizontal** | Difficile | Natif avec sharding |
| **Risk Analytics** | SQL complexe | MapReduce + Aggregation |

---

Ce design MongoDB Python est optimis√© pour votre architecture **risk-centric** avec des performances excellentes pour les corr√©lations complexes et le scoring en temps r√©el.